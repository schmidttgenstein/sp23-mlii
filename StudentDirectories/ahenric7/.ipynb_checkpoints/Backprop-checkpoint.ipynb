{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-da1e19bce38c>, line 170)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-da1e19bce38c>\"\u001b[0;36m, line \u001b[0;32m170\u001b[0m\n\u001b[0;31m    y_score = # grab the output of network\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import numpy.linalg as la \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGen:\n",
    "    def __init__(self,dim = 10,N = 20000):\n",
    "        self.dim = dim \n",
    "        self.N = N \n",
    "\n",
    "        \n",
    "    def gen_data(self,n = None,split = 0.6,mu_factor = 1):\n",
    "        x0,y0 = self.gen_label_data(label = 0,n_dat = n,mult = mu_factor)\n",
    "        x1,y1 = self.gen_label_data(label = 1,n_dat = n,mult = mu_factor)\n",
    "        cutoff = int(split * x0.shape[1])\n",
    "        x_tr = np.concatenate([x0[:,:cutoff],x1[:,:cutoff]],axis = 1)\n",
    "        y_tr = np.concatenate([y0[:,:cutoff],y1[:,:cutoff]],axis = 1)\n",
    "        x_te = np.concatenate([x0[:,cutoff:],x1[:,cutoff:]],axis = 1)\n",
    "        y_te = np.concatenate([y0[:,cutoff:],y1[:,cutoff:]],axis = 1)\n",
    "        return x_tr,y_tr,x_te,y_te\n",
    "\n",
    "    def gen_label_data(self,label = 0, n_dat = None,mult=1):\n",
    "        if n_dat is None:\n",
    "            n_dat = self.N \n",
    "        y = np.zeros([2,n_dat])\n",
    "        m = mult * np.random.random(self.dim) - .5 \n",
    "        C = np.random.random([self.dim,self.dim]) - .5\n",
    "        C = C @ np.transpose(C)\n",
    "        x = np.random.multivariate_normal(m, C, n_dat)\n",
    "        y[label,:] = 1\n",
    "        return np.transpose(x),y \n",
    "\n",
    "    def ym2yo(self,ym):\n",
    "        vals = 0 * ym\n",
    "        vals[1,:] = 1\n",
    "        idx1 = ym == 1\n",
    "        yo = vals[idx1]\n",
    "        return yo \n",
    "\n",
    "    def probe_dat(self,x,y,coord = 0):\n",
    "        yo = self.ym2yo(y)\n",
    "        idx1 = yo == 1 \n",
    "        x1 = x[:,idx1]\n",
    "        x0 = x[:,~idx1]\n",
    "        plt.hist(x0[coord,:],density = True, bins = 100,label = 'x(y=0)')\n",
    "        plt.hist(x1[coord,:],density = True, bins = 100,label = 'x(y=1)')\n",
    "        plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPipeline:\n",
    "    def __init__(self,epochs = 10,lr = 0.025):\n",
    "        ###In this constructor we set the model complexity, number of epochs for training, \n",
    "        ##and learning rate lr. You should think of complexity here as \"number of parameters\"\n",
    "        #defining model. In linear regression, this e.g. may be (deg of poly)-1. \n",
    "        self.epochs = epochs\n",
    "        self.lr = lr \n",
    "\n",
    "    def gen_data(self,):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def loss(self,):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self,):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def full_forward(self,):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    def backward(self,):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    def update(self,):\n",
    "        raise NotImplementedError  \n",
    "\n",
    "    def metrics(self,x,y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_step(self,x_in,y_truth):\n",
    "        '''\n",
    "            returning y_pred so that a redundant call to forward isn't \n",
    "            required \n",
    "        '''\n",
    "        ## need to properly define train_step (forward, backward, + update)\n",
    "        y_layers = [y_truth]\n",
    "        grad = [dw,db]\n",
    "        self.update(grad)\n",
    "        #do not absolutely need to return anything but it'll be convenient to have \n",
    "        # the model eval returned \n",
    "        return y_layers[-1]\n",
    "\n",
    "    def fit(self,x_data,y_data,x_eval,y_eval, printing = False):\n",
    "        ### This method implements our \"1. forward 2. backward 3. update paradigm\"\n",
    "        ## it should call forward(), grad(), and update(), in that order. \n",
    "        # you should also call metrics so that you may print progress during training\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.train_step(x_data,y_data)\n",
    "            if printing and (epoch % 25 == 0):\n",
    "                m_tr= self.metrics(y_pred,y_data)\n",
    "                y_pred_eval = self.forward(x_eval)\n",
    "                m_te = self.metrics(y_pred_eval,y_eval)\n",
    "                a_disc = np.abs(m_te[1] - m_tr[1])\n",
    "                l_disc = np.abs(m_te[0] - m_tr[0])\n",
    "                print(f\"epoch {epoch}: acc {m_te[1]:.3f}, acc discrep {a_disc:.3f}, loss disc {l_disc:.3f}\")\n",
    "\n",
    "\n",
    "class FCNetFS(MLPipeline):\n",
    "    def __init__(self,params:list=None, dims:list=None,epochs:int = 10,lr = .01,):\n",
    "        super().__init__(epochs = epochs,lr = lr)\n",
    "        if params is None:\n",
    "            weights = []\n",
    "            bias = []\n",
    "            for j in range(int(len(dims)-1)):\n",
    "                w = (np.random.random([dims[j+1],dims[j]])-.5)\n",
    "                b = (np.random.random(dims[j+1]) - .5)\n",
    "                weights.append(w)\n",
    "                bias.append(b)\n",
    "        else:\n",
    "            weights = params[0]\n",
    "            bias = params[1]\n",
    "            self.weights = weights \n",
    "            self.bias = bias\n",
    "            dims = [weights[0].shape[1]]\n",
    "            for w in weights: \n",
    "                dims.append(w.shape[0])\n",
    "        self.weights = weights \n",
    "        self.bias = bias \n",
    "        self.dims = dims \n",
    "        self.n_layers = len(dims)\n",
    "        self.lr = lr \n",
    "\n",
    "    def y_pred(self,input,inIsX = False,thresh = .5):\n",
    "        if inIsX:\n",
    "            y_score = self.forward(input)\n",
    "        else:\n",
    "            y_score = input \n",
    "        y_out = 0 * y_score\n",
    "        y_out[y_score >= thresh] = 1\n",
    "        return y_out.astype(int)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        la = self.full_forward(x)\n",
    "        a = la[-1]\n",
    "        return a\n",
    "   \n",
    "    def full_forward(self,x):\n",
    "        j = 0\n",
    "        a = x \n",
    "        layer_acts = [a]\n",
    "        for weight in self.weights:\n",
    "            z = weight @  a + self.bias[j].reshape([self.bias[j].shape[0],1])\n",
    "            a = self.activation(z)\n",
    "            layer_acts.append(a)\n",
    "            j+=1\n",
    "        return layer_acts\n",
    "\n",
    "    def backward(self,y_layers,y_truth):\n",
    "        ''' your work is here!:\n",
    "        y_layers = [a^0=x,a^1,...,a^L+1 = nu(x)]\n",
    "        '''\n",
    "        y_score = # grab the output of network\n",
    "        _,dc = #start with gradient of cost wrt output (should be 2 x m) \n",
    "        # we initialize lists to track gradients. \n",
    "        # note that parameters w and b are held in lists in the class \n",
    "        # w = [w^0,w^1,\\ldots] and similarly for b, corresponding to [z^0,z^1,...]\n",
    "        db_list = []\n",
    "        dw_list = []\n",
    "        for j in range(len(y_layers)-1):\n",
    "            ''' recalling that backprop is \"just\" the chain rule + keep track of some info \n",
    "            at each step, we're going to layer-wise compute \n",
    "            dc/dparam = dc/dnu * dnu / dz^j * dz^j / dparam.\n",
    "            We saw in class that we'll basically have a product of dz^{j+1}/dz^j\n",
    "            which is dz^{j+1} / da^{j+1} * da^{j+1} / dz^j. While this is true, \n",
    "            for the first step, we'll only use one of these, and therefore split the product\n",
    "            by computing the other half at the end of the loop\n",
    "            '''\n",
    "            idx = -(j+1) \n",
    "            y_layer = y_layers[idx]\n",
    "            dadz = #this is basically sigma'(z)\n",
    "            dc *= ## we are going to use dc to track the backprop, \n",
    "            ## i.e. dc is a glorified history of our use of the chain rule  \n",
    "            a_prev = ## we need the previous layer's activation for dc/dw\n",
    "            db = dc.mean(axis = 1) #I'll give you db \n",
    "            dw = # make sure you get the dimensions aligned. There may be some \n",
    "            #transposing required. Use of the debugger will come in handy here. \n",
    "            db_list.append(db)\n",
    "            dw_list.append(dw)\n",
    "            weight = self.weights[idx]\n",
    "            dc = ## Now you can use the other half of the dz^j+1/dz^j computation \n",
    "            ## recall that we are mapping R^{n_{j+1}} --> R^{n_j}!, so you might have \n",
    "            # or need another judicious use of transpose \n",
    "        # reversing the list because in gradient update we'll iterate through params, \n",
    "        # and they're stored starting with early layers first\n",
    "        db_list.reverse()\n",
    "        dw_list.reverse()\n",
    "        grad = zip(dw_list,db_list)\n",
    "        return grad\n",
    "\n",
    "    def update(self,grad):\n",
    "        j = 0 \n",
    "        for dw,db in grad:\n",
    "            self.weights[j]  = self.weights[j] - self.lr * dw \n",
    "            self.bias[j] = self.bias[j] - self.lr * db \n",
    "            j += 1\n",
    "        return None\n",
    "\n",
    "\n",
    "    def metrics(self,x,y):\n",
    "        return None\n",
    "\n",
    "    def activation(self,z):\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def cost(self,y_score,y_truth):\n",
    "        # your cost function should return cross entropy cost (sum over classes)\n",
    "        c = -y_truth*np.log(y_score) -(1-y_truth)*np.log(1-y_score)\n",
    "        dcdys  = -y_truth/y_score + (1-y_truth)/(1-y_score)\n",
    "        return c.sum(axis = 0),dcdys\n",
    "    \n",
    "    def metrics(self,y_score,y_truth):\n",
    "        y_pred = self.y_pred(y_score)\n",
    "        loss = self.loss(y_score,y_truth)\n",
    "        acc = (y_pred == y_truth).mean()\n",
    "        return loss, acc\n",
    "    \n",
    "    def loss(self,y_sc,y_truth):\n",
    "        ### calculating loss using partial as initial computation\n",
    "        c,_ = self.cost(y_sc,y_truth)\n",
    "        return c.mean()\n",
    "\n",
    "    def l_grad(self,x_in,y_truth):\n",
    "        ### partial loss / partial y_pred \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 0: generate data and look at class separation \n",
    "    input_dim = 1\n",
    "    dg = DataGen(dim = input_dim)\n",
    "    ###a larger mu_factor will better separate means \n",
    "    ## and therefore class data \n",
    "    # We'll keep this large before getting backprop running\n",
    "    xtr,ytr,xte,yte = dg.gen_data(mu_factor = 5)\n",
    "    '''\n",
    "    dg.probe_dat(xtr,ytr)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    '''\n",
    "\n",
    "    # Step 1: instantiate 1 hidden layer nn for binary classification task \n",
    "    # and define cost function returning cost + d cost/d nu \n",
    "    anet = FCNetFS(dims = [input_dim,20,2],epochs = 150,lr = 1)\n",
    "    #sanity check that dcdnu is pointed in the right direction! \n",
    "    y_score = anet.forward(xtr)\n",
    "    c,dc = anet.cost(y_score,ytr)\n",
    "    ### figure out whether the following is + or - \n",
    "    ##... and don't be lazy, don't *just* try each and see what\n",
    "    # catches the following 'if' statement \n",
    "    dc_dir = 0 \n",
    "    #dc_dir  = (np.sign(dc) == +/- np.sign(ytr - y_score)).mean()\n",
    "    if dc_dir == 1:\n",
    "        print('dc/dnu points in the right direction!')\n",
    "\n",
    "    #### Step 2: define backward() method in FCNetFS and\n",
    "    ### train_step() (forward, backward, update) in MLPipeline\n",
    "    ## if prob_dat() shows a plot with separated classes, your \n",
    "    # accuracy should hit 100%\n",
    "    #anet.fit(xtr,ytr,xte,yte,printing = True)\n",
    " \n",
    "    ### Step 3: let's make things harder, just check that you can \n",
    "    ## still construct a good model\n",
    "    input_dim = 15\n",
    "    dg = DataGen(dim = input_dim)\n",
    "    xtr,ytr,xte,yte = dg.gen_data(mu_factor = .5)\n",
    "    # you may not see much now separation with prob_dat()\n",
    "    '''\n",
    "    for j in range(input_dim):\n",
    "            plt.figure(j)\n",
    "            dg.probe_dat(xtr,ytr,j)\n",
    "    plt.show()\n",
    "    bnet = FCNetFS(dims = [input_dim,20,15,10,2],epochs = 500,lr = 1)\n",
    "    bnet.fit(xtr,ytr,xte,yte,printing = True)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
